{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型介绍\n",
    "\n",
    "**Deep Layer Aggregation**\n",
    "\n",
    "github pytorch代码: [https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/dla.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/dla.py)\n",
    "\n",
    "论文地址: [https://arxiv.org/pdf/1707.06484.pdf](https://arxiv.org/pdf/1707.06484.pdf)\n",
    "- 作者探讨了深度网络的跨层信息融合， skip connection一定成都也浅化了自身网络。\n",
    "- 更多非线性，较大网络容量和更大的感受野一方面提升精度，但是对优化和计算比较麻烦。\n",
    "- 作者提出的想法是结合FPN和densenet， 从垂直到迭代短连接，深化表示层和分辨率。\n",
    "- 设计两种层聚合，包括迭代深度聚合(IDA)以及层级聚合(HDA)。IDA主要是融合分辨率和尺度(空间信息where)， HDA主要是融合各个模块和通道的特征(语义信息what)， 最终提升了性能，参数量和内存使用。\n",
    "- 尝试的不同聚合方式，b. 只是跟最后一层进行融合，通常用于语义分割和目标检测，c. 中间层迭代短连接，循环使用最浅的网络层， d.属性结构跨不同深度，e和f进一步优化d，将中间聚合结果导回到原来的网络结构，在同一深度连续融合提升效果。本文主要基于c和f进行识别和分解。\n",
    "- HDA一定程度上解决了梯度消失和爆炸的问题， 残差链接只有在4层以上的网络有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/0e135a135ec144feb2aed45a59ee89aa4d9ee334fb8244d79553233e96477bb4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 关于数据集ImageNet\n",
    "\n",
    "ImageNet图像数据集始于2009年，当时李飞飞教授等在CVPR2009上发表了一篇名为《ImageNet: A Large-Scale Hierarchical Image Database》的论文，之后就是基于ImageNet数据集的7届ImageNet挑战赛(2010年开始)，2017年后，ImageNet由Kaggle(Kaggle公司是由联合创始人兼首席执行官Anthony Goldbloom 2010年在墨尔本创立的，主要是为开发商和数据科学家提供举办机器学习竞赛、托管数据库、编写和分享代码的平台)继续维护。\n",
    "\n",
    "本AIStudio项目在线下进行的训练， 所以只使用了验证集进行验证\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1e8613aebb754b96bc799dd3c0c51278da5ab0599e264467912c9e2782821a24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#数据集解压\n",
    "!mkdir ~/data/ILSVRC2012\n",
    "!tar -xf ~/data/data68594/ILSVRC2012_img_val.tar -C ~/data/ILSVRC2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载数据集\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.io import Dataset\n",
    "from paddle.vision.datasets import DatasetFolder, ImageFolder\n",
    "# from paddle.vision.transforms import Compose, Resize, Transpose, Normalize\n",
    "import paddle.vision.transforms as T\n",
    "train_parameters = {\n",
    "    'train_image_dir': '/home/aistudio/data/ILSVRC2012',\n",
    "    'eval_image_dir': '/home/aistudio/data/ILSVRC2012',\n",
    "    'test_image_dir': '/home/aistudio/data/ILSVRC2012',\n",
    "}\n",
    "\n",
    "class CatDataset(Dataset):\n",
    "    def __init__(self, mode='train'):\n",
    "        super(CatDataset, self).__init__()\n",
    "        train_image_dir = train_parameters['train_image_dir']\n",
    "        eval_image_dir = train_parameters['eval_image_dir']\n",
    "        test_image_dir = train_parameters['test_image_dir']\n",
    "\n",
    "        data_transforms = T.Compose([\n",
    "            T.Resize(256, interpolation='bicubic'),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        train_data_folder = DatasetFolder(train_image_dir, transform=data_transforms)\n",
    "        eval_data_folder = DatasetFolder(eval_image_dir, transform=data_transforms)\n",
    "        test_data_folder = ImageFolder(test_image_dir, transform=data_transforms)\n",
    "        self.mode = mode\n",
    "        if self.mode  == 'train':\n",
    "            self.data = train_data_folder\n",
    "        elif self.mode  == 'eval':\n",
    "            self.data = eval_data_folder\n",
    "        elif self.mode  == 'test':\n",
    "            self.data = test_data_folder\n",
    "        print(mode, len(self.data))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index][0].astype('float32')\n",
    "        if self.mode  == 'test':\n",
    "            return data\n",
    "        else:\n",
    "            label = np.array([self.data[index][1]]).astype('int64')\n",
    "            return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型结构搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.vision.transforms as T\n",
    "from PIL import Image\n",
    "import paddle.optimizer as opt\n",
    "import paddle.distributed as dist\n",
    "import paddle\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3\"\n",
    "paddle.set_device('gpu:0')\n",
    "\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from math import ceil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#DLA瓶颈模块\n",
    "class DlaBottleneck(nn.Layer):\n",
    "    \"\"\"DLA/DLA-X Bottleneck\"\"\"\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, stride=1, dilation=1, cardinality=1, base_width=64):\n",
    "        super(DlaBottleneck, self).__init__()\n",
    "\n",
    "        self.stride = stride\n",
    "        mid_planes = int(math.floor(outplanes * (base_width / 64)) * cardinality)\n",
    "        mid_planes = mid_planes // self.expansion\n",
    "\n",
    "        self.conv1 = nn.Conv2D(inplanes, mid_planes, kernel_size=1, bias_attr=False)\n",
    "        self.bn1 = nn.BatchNorm2D(mid_planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2D(\n",
    "            mid_planes, mid_planes, kernel_size=3, stride=stride, padding=dilation,\n",
    "            bias_attr=False, dilation=dilation, groups=cardinality) #tocheck\n",
    "\n",
    "        if stride == 2:\n",
    "            self.conv2 = nn.Conv2D(\n",
    "                mid_planes, mid_planes, kernel_size=3, stride=stride, padding=[1,0,1,0],\n",
    "                bias_attr=False, dilation=dilation, groups=cardinality)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2D(mid_planes)\n",
    "\n",
    "        self.conv3 = nn.Conv2D(mid_planes, outplanes, kernel_size=1, bias_attr=False)\n",
    "        self.bn3 = nn.BatchNorm2D(outplanes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.outplanes = outplanes\n",
    "\n",
    "    def forward(self, x, residual=None):\n",
    "        if residual is None:\n",
    "            residual = x\n",
    "\n",
    "        out = self.conv1(x)  #断点测试OK\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out) #确实存在明显差异， 断点测试是对齐，但是整齐跑就会有问题\n",
    "\n",
    "        out = self.conv3(out)#断点测试OK\n",
    "        out = self.bn3(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#没找到对应接口，手写一个\n",
    "class Identity(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DlaRoot(nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, residual):\n",
    "        super(DlaRoot, self).__init__()\n",
    "        self.conv = nn.Conv2D(\n",
    "            in_channels, out_channels, 1, stride=1, bias_attr=False, padding=(kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2D(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, *x):\n",
    "        children = x\n",
    "        x = self.conv(paddle.concat(x, axis=1))\n",
    "        x = self.bn(x)\n",
    "        if self.residual:\n",
    "            x += children[0]\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#树状连接\n",
    "class DlaTree(nn.Layer):\n",
    "    def __init__(self, levels, block, in_channels, out_channels, stride=1, dilation=1, cardinality=1, base_width=64,\n",
    "                 level_root=False, root_dim=0, root_kernel_size=1, root_residual=False):\n",
    "        super(DlaTree, self).__init__()\n",
    "        if root_dim == 0:\n",
    "            root_dim = 2 * out_channels\n",
    "        if level_root:\n",
    "            root_dim += in_channels\n",
    "\n",
    "        self.downsample = nn.MaxPool2D(stride, stride=stride) if stride > 1 else Identity()\n",
    "        self.project = Identity()\n",
    "\n",
    "        cargs = dict(dilation=dilation, cardinality=cardinality, base_width=base_width)\n",
    "        if levels == 1:\n",
    "            self.tree1 = block(in_channels, out_channels, stride, **cargs)\n",
    "            self.tree2 = block(out_channels, out_channels, 1, **cargs)\n",
    "            if in_channels != out_channels:\n",
    "                self.project = nn.Sequential(\n",
    "                    nn.Conv2D(in_channels, out_channels, kernel_size=1, stride=1, bias_attr=False),\n",
    "                    nn.BatchNorm2D(out_channels))\n",
    "        else:\n",
    "            cargs.update(dict(root_kernel_size=root_kernel_size, root_residual=root_residual))\n",
    "            self.tree1 = DlaTree(\n",
    "                levels - 1, block, in_channels, out_channels, stride, root_dim=0, **cargs)\n",
    "            self.tree2 = DlaTree(\n",
    "                levels - 1, block, out_channels, out_channels, root_dim=root_dim + out_channels, **cargs)\n",
    "\n",
    "        if levels == 1:\n",
    "            self.root = DlaRoot(root_dim, out_channels, root_kernel_size, root_residual)\n",
    "\n",
    "        self.level_root = level_root\n",
    "        self.root_dim = root_dim\n",
    "        self.levels = levels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x, residual=None, children=None):\n",
    "        children = [] if children is None else children\n",
    "        bottom = self.downsample(x)\n",
    "        residual = self.project(bottom)\n",
    "        if self.level_root:\n",
    "            children.append(bottom)\n",
    "        x1 = self.tree1(x, residual)\n",
    "\n",
    "        if self.levels == 1:\n",
    "            x2 = self.tree2(x1) \n",
    "            x = self.root(x2, x1, *children)\n",
    "        else:\n",
    "            if self.out_channels == 1024:\n",
    "                print(\"debuging....2\")\n",
    "            children.append(x1)\n",
    "            x = self.tree2(x1, children=children)\n",
    "        return x\n",
    "\n",
    "#DLA60模型\n",
    "class DLA60(nn.Layer):\n",
    "    def __init__(self, output_stride=32, num_classes=1000, in_chans=3,\n",
    "                 cardinality=1, base_width=64, block=DlaBottleneck, residual_root=False,\n",
    "                 drop_rate=0.0, global_pool='avg'):\n",
    "        super(DLA60, self).__init__()\n",
    "\n",
    "        #dla60配置\n",
    "        levels=[1, 1, 1, 2, 3, 1]\n",
    "        channels=[16, 32, 128, 256, 512, 1024]\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "        self.cardinality = cardinality\n",
    "        self.base_width = base_width\n",
    "        self.drop_rate = drop_rate\n",
    "        assert output_stride == 32  # FIXME support dilation\n",
    "\n",
    "        self.base_layer = nn.Sequential(\n",
    "            nn.Conv2D(in_chans, channels[0], 7, 1, 3, bias_attr=False),\n",
    "            nn.BatchNorm2D(channels[0]),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.level0 = self._make_conv_level(channels[0], channels[0], levels[0])\n",
    "        self.level1 = self._make_conv_level(channels[0], channels[1], levels[1], stride=2)\n",
    "\n",
    "        cargs = dict(cardinality=cardinality, base_width=base_width, root_residual=residual_root)\n",
    "        self.level2 = DlaTree(levels[2], block, channels[1], channels[2], 2, level_root=False, **cargs)\n",
    "        self.level3 = DlaTree(levels[3], block, channels[2], channels[3], 2, level_root=True, **cargs)\n",
    "        self.level4 = DlaTree(levels[4], block, channels[3], channels[4], 2, level_root=True, **cargs)\n",
    "        self.level5 = DlaTree(levels[5], block, channels[4], channels[5], 2, level_root=True, **cargs)\n",
    "\n",
    "        self.feature_info = [\n",
    "            dict(num_chs=channels[0], reduction=1, module='level0'),  # rare to have a meaningful stride 1 level\n",
    "            dict(num_chs=channels[1], reduction=2, module='level1'),\n",
    "            dict(num_chs=channels[2], reduction=4, module='level2'),\n",
    "            dict(num_chs=channels[3], reduction=8, module='level3'),\n",
    "            dict(num_chs=channels[4], reduction=16, module='level4'),\n",
    "            dict(num_chs=channels[5], reduction=32, module='level5'),\n",
    "        ]\n",
    "\n",
    "        self.num_features = channels[-1]\n",
    "        self.pool = nn.AdaptiveAvgPool2D(1)\n",
    "        self.fc = nn.Conv2D(self.num_features, num_classes, 1,  bias_attr=True)\n",
    "\n",
    "    def _make_conv_level(self, inplanes, planes, convs, stride=1, dilation=1):\n",
    "        modules = []\n",
    "        for i in range(convs):\n",
    "            modules.extend([\n",
    "                nn.Conv2D(inplanes, planes, 3, stride if i == 0 else 1,\n",
    "                        dilation, bias_attr=False, dilation=dilation),\n",
    "                nn.BatchNorm2D(planes),\n",
    "                nn.ReLU()])\n",
    "\n",
    "            inplanes = planes\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.base_layer(x) \n",
    "        x = self.level0(x)\n",
    "        x = self.level1(x)\n",
    "        x = self.level2(x)\n",
    "        x = self.level3(x)\n",
    "        x = self.level4(x)\n",
    "        x = self.level5(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.pool(x)\n",
    "        if self.drop_rate > 0.:\n",
    "            x = F.dropout(x, dropout_prob=self.drop_rate)\n",
    "        x = self.fc(x)\n",
    "        x = x.flatten(1)  # conv classifier, flatten if pooling isn't pass-through (disabled)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  精度对齐\n",
    "\n",
    "因为是简单的图像分类模型，这里只做一个相同输入下的输出结果验证\n",
    "\n",
    "**torch的输出**\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/97745d1de1414aa9ad49fb6ee47208d8fe460e9cd1fd4968947b331a03ef1a10)\n",
    "\n",
    "**paddle的输出**\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/8595a35a04d94012b60c2f71360a74766445bde9520f46199515aef0ac4f772d)\n",
    "\n",
    "**验证集上验证**\n",
    "\n",
    "step 98/98 [==============================] - loss: 0.7075 - acc: 0.7682 - 11s/step 离提交要求还有轻微距离， 要求是0.769 训练时间比较慢近5h一轮\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 训练模型\n",
    "\n",
    "由于训练集特别大, AIStduio暂时还受不了, 这里只用验证集数据训练了两轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 在AIStuido里测试时加载的数据集\r\n",
    "import cv2\r\n",
    "transforms = T.Compose([\r\n",
    "    T.Resize(256, interpolation='bicubic'),\r\n",
    "    T.CenterCrop(224),\r\n",
    "    T.ToTensor(),\r\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n",
    "])\r\n",
    "\r\n",
    "# 构建数据集\r\n",
    "class ILSVRC2012(paddle.io.Dataset):\r\n",
    "    def __init__(self, root, label_list, transform, backend='pil'):\r\n",
    "        self.transform = transform\r\n",
    "        self.root = root\r\n",
    "        self.label_list = label_list\r\n",
    "        self.backend = backend\r\n",
    "        self.load_datas()\r\n",
    "\r\n",
    "    def load_datas(self):\r\n",
    "        self.imgs = []\r\n",
    "        self.labels = []\r\n",
    "        with open(self.label_list, 'r') as f:\r\n",
    "            for line in f:\r\n",
    "                img, label = line[:-1].split(' ')\r\n",
    "                self.imgs.append(os.path.join(self.root, img))\r\n",
    "                self.labels.append(int(label))\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        label = self.labels[idx]\r\n",
    "        image = self.imgs[idx]\r\n",
    "        if self.backend=='cv2':\r\n",
    "            image = cv2.imread(image)\r\n",
    "        else:\r\n",
    "            image = Image.open(image).convert('RGB')\r\n",
    "        image = self.transform(image)\r\n",
    "        return image.astype('float32'), np.array(label).astype('int64')\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.imgs)\r\n",
    "\r\n",
    "val_dataset = ILSVRC2012('data/ILSVRC2012', transform=transforms, label_list='data/data68594/val_list.txt')\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:143: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data.dtype == np.object:\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:648: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 196/196 [==============================] - loss: 7.0864 - acc: 9.2000e-04 - 3s/step          \n",
      "save checkpoint at /home/aistudio/checkpoints/0\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 196/196 [==============================] - loss: 7.0807 - acc: 8.6000e-04 - 2s/step         \n",
      "Eval samples: 50000\n",
      "Epoch 2/2\n",
      "step 196/196 [==============================] - loss: 6.9930 - acc: 8.8000e-04 - 3s/step          \n",
      "save checkpoint at /home/aistudio/checkpoints/1\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 196/196 [==============================] - loss: 7.0972 - acc: 0.0011 - 2s/step         \n",
      "Eval samples: 50000\n",
      "save checkpoint at /home/aistudio/checkpoints/final\n"
     ]
    }
   ],
   "source": [
    "#保存训练结果\n",
    "callback = paddle.callbacks.ModelCheckpoint(save_dir='./checkpoints', save_freq=1)\n",
    "\n",
    "#加载模型及预训练参数\n",
    "model = DLA60(num_classes=1000)\n",
    "run_model = paddle.Model(model)\n",
    "\n",
    "#模型训练\n",
    "optim = paddle.optimizer.SGD(learning_rate=0.0001, weight_decay=6e-5, parameters=run_model.parameters())\n",
    "run_model.prepare(optimizer= optim,\n",
    "              loss=paddle.nn.CrossEntropyLoss(),\n",
    "              metrics=paddle.metric.Accuracy())\n",
    "run_model.fit(val_dataset, val_dataset, epochs=2, batch_size=256, callbacks=callback, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 验证模型\n",
    "\n",
    "模型验证的结果非常接近论文的精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 98/98 [==============================] - loss: 1.0160 - acc: 0.7664 - 6s/step         \n",
      "Eval samples: 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [1.015959], 'acc': 0.76638}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DLA60(num_classes=1000)\r\n",
    "model_state_dict = paddle.load(\"/home/aistudio/work/dla60_best.pdparams\")\r\n",
    "model.set_state_dict(model_state_dict)\r\n",
    "run_model = paddle.Model(model)\r\n",
    "optim = paddle.optimizer.SGD(learning_rate=0.0001, weight_decay=6e-5, parameters=run_model.parameters())\r\n",
    "run_model.prepare(optimizer= optim,\r\n",
    "              loss=paddle.nn.CrossEntropyLoss(),\r\n",
    "              metrics=paddle.metric.Accuracy())\r\n",
    "\r\n",
    "#模型验证\r\n",
    "run_model.evaluate(val_dataset, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 总结\n",
    "\n",
    "因训练硬件资源和时间有限，本次复现过程还有很多缺失和不足，后续持续改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
